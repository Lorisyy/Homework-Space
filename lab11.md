#Lab11 IT伦理与道德研究(By google scholar and wikipedia)
## 宿永烨 18342107
## Ethics In Self-driving cars
### What is self-driving technology ?
In general self-driving refers to a system that directly in charge of the following things : vehicles,ships,planes,submarines and so on.  
According to the wikipedia:
>A self-driving car, also known as a robot car, autonomous car, or driverless car, is a vehicle that is capable of sensing its environment and moving with little or no human input.

>Autonomous cars combine a variety of sensors to perceive their surroundings, such as radar, computer version ,Lidar, sonar, GPS, odometry and inertial measurement units. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage.

We can know what the self-driving exactly is.  

![A Self-driving car](images/car.)

### How it is applied to our lives?
![Scottish](images/bridge.png)

According to THE GUARDIAN(a famous British media):  
>The first driverless public transport services on the UK’s roads will be in action by 2021, the government has announced, led by an autonomous bus service crossing the Forth estuary in Scotland, and self-driving taxis in up to four London boroughs.

>The services should be on the roads within three years, under three pilot schemes announced by the Department for Business, Energy and Industrial Strategy. Full-sized driverless buses will run on a 14-mile route between Fife and Edinburgh, while Addison Lee and Jaguar Land Rover will lead two separate trials of driverless taxis in London before launching public services, again by 2021.

>The Scottish driverless bus service could provide up to 10,000 journeys a week across the Forth road bridge. Five single-decker buses, carrying up to 42 passengers, will be converted from manually driven to autonomous vehicles. The scheme will involve organisations from across the UK and will be led by Fusion Processing, a technology company which specialises in sensors and control systems.

**Believe it or not , as you can see here,it is coming very soon !**

### What is the ethic problem in self-driving devices,espeacially the one which close to our daily lives?

>Consider this hypothetical:  
>It’s a bright, sunny day and you’re alone in your spanking new self-driving vehicle, sprinting along the two-lane Tunnel of Trees on M-119 high above Lake Michigan north of Harbor Springs. You’re sitting back, enjoying the view. You’re looking out through the trees, trying to get a glimpse of the crystal blue water below you, moving along at the 45-m.p.h. speed limit.

>As you approach a rise in the road, heading south, a school bus appears, driving north, one driven by a human, and it veers sharply toward you. There is no time to stop safely, and no time for you to take control of the car.

The Question is OBVIOUS,
Does the car:  

A. Swerve sharply into the trees, possibly killing you but possibly saving the bus and its occupants?  

B. Perform a sharp evasive maneuver around the bus and into the oncoming lane, possibly saving you, but sending the bus and its driver swerving into the trees, killing her and some of the children on board?  

C. Hit the bus, possibly killing you as well as the driver and kids on the bus?  

###  As Patrick Lin i,Detroit Free Press, researched:

In everyday driving, such no-win choices are may be exceedingly rare but, when they happen, what should a self-driving car — programmed in advance — do? Or in any situation — even a less dire one — where a moral snap judgment must be made?  

As a philosopher working with engineers in Stanford’s Center for Automotive Research, I was initially surprised that we spent our lab meetings discussing what I thought was an easy question: How should a self-driving car approach a crosswalk?

My assumption had been that we would think about how a car should decide between the lives of its passengers and the lives of pedestrians. I knew how to think about such dilemmas because these crash scenarios resemble a famous philosophical brainteaser called the “trolley problem.” Imagine a runaway trolley is hurling down the tracks and is bound to hit either a group of five or a single person – would you kill one to save five?

However, many philosophers nowadays doubt that investigating such questions is a fruitful avenue of research. Barbara Fried, a colleague at Stanford, for example, has argued that tragic dilemmas make people believe ethical quandaries mostly arise in extreme and dire circumstances.

In fact, ethical quandaries are ubiquitous. Everyday, mundane situations are surprisingly messy and complex, often in subtle ways. For example: Should your city spend money on a diabetes prevention program or on more social workers? Should your local Department of Public Health hire another inspector for restaurant hygiene standards, or continue a program providing free needles and injection supplies?

These questions are extremely difficult to answer because of uncertainties about the consequences – such as who will be affected and to what degree. The solutions philosophers have proposed for extreme and desperate situations are of little help here.

The problem is similar with self-driving cars. Thinking through extreme situations and crash scenarios cannot help answer questions that arise in mundane situations.  

An article pubulished in :[IEEE Spectrum](https://ieeexplore.ieee.org/abstract/document/7473149)  also emphasized this.

All in all,we must know that :
**ALL DRIVING INVOLVES RISK**

### Another difference between ROBOT ethics and the human kind is that theirs can be wraped,even by programmmers who had only the best of intentions.
>Article pubulished in :[IEEE Spectrum](https://ieeexplore.ieee.org/abstract/document/7473149)  

Imagine that the algorithm operating a driverless car adjusted the buffering space it assigns to pedestrians in different districts, which it might identify by analyzing settlements from civil proceedings involving crashes. Although this is a perfectly reasonable, well-intentioned, and efficient way of controlling a vehicle's behavior, it can also lead to bad outcomes if, for example, the actual reasons injured pedestrians settled for less were because they lived in low-income neighborhoods. The algorithm would then inadvertently penalize the poor by providing them smaller buffers and slightly increasing their risk of being hit when out for a walk.

It is tempting to dismiss such concerns as idle academic maunderings, but there is no way around them, because computer programs take things quite literally. The time to figure out the consequences of an action is before they happen—in design, rather than the patchin phase.

And this is partly why so many researchers use hypothetical situations in which the vehicle must decide between two or more bad outcomes. The most famous of these is the “trolley problem,” in which a trolley is threatening to collide with unsuspecting children and the only way to stop it is to throw a fat man over the side of a bridge and onto the track's switch. (The man's weight matters: Otherwise, a self-sacrificing onlooker could jump off the bridge himself.) Do you sacrifice one life to save many by such a positive action? If your answer is “no,” consider this: You'd no doubt be willing to sacrifice one life to save many by refusing to act—so how can you justify the apparent contradiction?

There is a substantial literature on such thought experiments, and indeed, they allow you to stress-test simple and straightforward ethics systems and to find areas where a bit more nuance would be helpful. Suppose an automated vehicle were programmed to avoid pedestrians at all costs. If a pedestrian were to suddenly appear in a two-lane tunnel, and the vehicle couldn't stop in time, the vehicle would be forced to swerve, even into the path of an oncoming bus loaded with passengers. The plausibility of that specific scenario is less important than the flaw it exposes in the vehicle's logic—that valuing pedestrian safety as categorically more important than that of any other road users can actually be much more dangerous in certain situations.

The ethics of road-vehicle automation is a solvable problem. We know this because other fields have handled comparable risks and benefits in a safe and reasonable way. Donated organs are distributed to the sick based on metrics based on quality-adjusted life years and disability-adjusted life years, among other variables. And the military draft has added exemptions for certain useful professions, such as farmer and teacher.

Automated vehicles face a greater challenge. They must decide quickly, with incomplete information, in situations that programmers often will not have considered, using ethics that must be encoded all too literally in software. Fortunately, the public doesn't expect superhuman wisdom but rather a rational justification for a vehicle's actions that considers the ethical implications. A solution doesn't need to be perfect, but it should be thoughtful and defensible.

